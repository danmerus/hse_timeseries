{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader, Dataset, IterableDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch import optim\n",
    "import math\n",
    "from math import sqrt\n",
    "import time \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "print(get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "data = pd.read_csv('data/ETTh1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length:  17420\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>5.827</td>\n",
       "      <td>2.009</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.462</td>\n",
       "      <td>4.203</td>\n",
       "      <td>1.340</td>\n",
       "      <td>30.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>5.693</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.142</td>\n",
       "      <td>1.371</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 02:00:00</td>\n",
       "      <td>5.157</td>\n",
       "      <td>1.741</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.355</td>\n",
       "      <td>3.777</td>\n",
       "      <td>1.218</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date   HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
       "0  2016-07-01 00:00:00  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
       "1  2016-07-01 01:00:00  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
       "2  2016-07-01 02:00:00  5.157  1.741  1.279  0.355  3.777  1.218  27.787001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('dataset length: ',len(data))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EttDataset(Dataset): # add scaling\n",
    "    def __init__(self, data, predict_for=24*4, flag=0):\n",
    "        self.seq_len = 24*4*4\n",
    "        self.label_len = 24*4\n",
    "        self.pred_len = predict_for\n",
    "        self.flag = flag\n",
    "        self.data = data\n",
    "        \n",
    "        border1s = [0, 12*30*24 - self.seq_len, 12*30*24+4*30*24 - self.seq_len]\n",
    "        border2s = [12*30*24, 12*30*24+4*30*24, 12*30*24+8*30*24]\n",
    "        border1 = border1s[self.flag]\n",
    "        border2 = border2s[self.flag]\n",
    "        \n",
    "        cols_data = self.data.columns[1:]\n",
    "        df_data = self.data[cols_data]\n",
    "            \n",
    "        df_stamp = self.data[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        \n",
    "        df_stamp['month'] = df_stamp.date.apply(lambda row:row.month,1)\n",
    "        df_stamp['day'] = df_stamp.date.apply(lambda row:row.day,1)\n",
    "        df_stamp['weekday'] = df_stamp.date.apply(lambda row:row.weekday(),1)\n",
    "        df_stamp['hour'] = df_stamp.date.apply(lambda row:row.hour,1)\n",
    "        df_stamp['minute'] = df_stamp.date.apply(lambda row:row.minute,1)\n",
    "        df_stamp['minute'] = df_stamp.minute.map(lambda x:x//15)\n",
    "        freq_map = {\n",
    "            'y':[],'m':['month'],'w':['month'],'d':['month','day','weekday'],\n",
    "            'b':['month','day','weekday'],'h':['month','day','weekday','hour'],\n",
    "            't':['month','day','weekday','hour','minute'],\n",
    "        }\n",
    "        \n",
    "        self.data_stamp = df_stamp[freq_map['h']].values\n",
    "        data_vals = df_data.values\n",
    "        self.data_y = data_vals[border1:border2]\n",
    "        self.data_x = data_vals[border1:border2]\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len \n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 3:1:1 ratio\n",
    "train_size = int((0.6)*len(data))\n",
    "val_size = int((0.2)*len(data))\n",
    "test_size = len(data)-val_size-train_size\n",
    "train_d, val_d, test_d = data[:train_size], data[train_size:train_size+val_size], data[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10452, 3484, 3484)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_d), len(val_d), len(test_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model, \n",
    "                                    kernel_size=3, padding=padding, padding_mode='circular')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
    "        return x\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4; hour_size = 24\n",
    "        weekday_size = 7; day_size = 32; month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type=='fixed' else nn.Embedding\n",
    "        if freq=='t':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        \n",
    "        minute_x = self.minute_embed(x[:,:,4]) if hasattr(self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:,:,3])\n",
    "        weekday_x = self.weekday_embed(x[:,:,2])\n",
    "        day_x = self.day_embed(x[:,:,1])\n",
    "        month_x = self.month_embed(x[:,:,0])\n",
    "        \n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h':4, 't':5, 's':6, 'm':1, 'a':1, 'w':2, 'd':3, 'b':3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq) if embed_type!='timeF' else TimeFeatureEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        print('value_embedding: ', self.value_embedding(x))\n",
    "        x = self.value_embedding(x) + self.position_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedEmbedder(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedder, self).__init__()\n",
    "        \n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.c_in = c_in\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.c_in, out_channels=self.d_model, kernel_size=3, padding=1, padding_mode='circular')\n",
    "        \n",
    "    def forward(self, x, x_mark):\n",
    "        #x0 = x.clone().detach()\n",
    "        # value\n",
    "        value_embedding = self.conv1(x.permute(0, 2, 1)).transpose(1,2)\n",
    "        print('value_embedding: ', value_embedding)\n",
    "        # position\n",
    "        max_len=5000\n",
    "        temp = torch.zeros(max_len, self.d_model).float()\n",
    "        temp.require_grad = False\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, self.d_model, 2).float() * -(math.log(10000.0) / self.d_model)).exp()\n",
    "        temp[:, 0::2] = torch.sin(position * div_term)\n",
    "        temp[:, 1::2] = torch.cos(position * div_term)\n",
    "        temp = temp.unsqueeze(0)\n",
    "        position_embedding = temp[:, :x.size(1)]\n",
    "        # timestamp\n",
    "        minute_size = 4; hour_size = 24; weekday_size = 7; day_size = 32; month_size = 13\n",
    "        hour_embed = FixedEmbedder(hour_size, self.d_model)\n",
    "        weekday_embed = FixedEmbedder(weekday_size, self.d_model)\n",
    "        day_embed = FixedEmbedder(day_size, self.d_model)\n",
    "        month_embed = FixedEmbedder(month_size, self.d_model)\n",
    "        x1 = x_mark.long()\n",
    "        hour_x = hour_embed(x1[:,:,3])\n",
    "        weekday_x = weekday_embed(x1[:,:,2])\n",
    "        day_x = day_embed(x1[:,:,1])\n",
    "        month_x = month_embed(x1[:,:,0])\n",
    "        temporal_embedding= hour_x + weekday_x + day_x + month_x + minute_x\n",
    "        \n",
    "        \n",
    "        x = value_embedding + position_embedding + temporal_embedding\n",
    "        \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
